diff --git a/ivybuild.xml b/ivybuild.xml
new file mode 100644
index 0000000..0804f47
--- /dev/null
+++ b/ivybuild.xml
@@ -0,0 +1,270 @@
+<?xml version="1.0"?>
+<project name="hadoop-hdfs" default="published"
+  xmlns:ivy="antlib:org.apache.ivy.ant">
+<!--
+   Licensed to the Apache Software Foundation (ASF) under one or more
+   contributor license agreements.  See the NOTICE file distributed with
+   this work for additional information regarding copyright ownership.
+   The ASF licenses this file to You under the Apache License, Version 2.0
+   (the "License"); you may not use this file except in compliance with
+   the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+-->
+
+  <description>
+    This is a build file to publish Hadoop hdfs as ivy and maven artifacts.
+    It currently works alongside the original build.xml file, and exists
+    purely to hook up hadoop into the SmartFrog test/release process.
+  </description>
+
+  <!--Override point: allow for overridden in properties to be loaded-->
+  <property file="build.properties" />
+  <property file="../build.properties" />
+
+
+  <target name="ivy-init-properties" >
+    <property name="ivy.dir" location="ivy" />
+    <loadproperties srcfile="${ivy.dir}/libraries.properties" />
+    <property name="ivysettings.xml" location="${ivy.dir}/ivysettings.xml" />
+    <property name="ivy.jar" location="${ivy.dir}/ivy-${ivy.version}.jar"/>
+    <property name="ivy.org" value="org.apache.hadoop"/>
+
+    <property name="build.dir" location="build" />
+    <property name="build.ivy.dir" location="${build.dir}/ivy" />
+    <property name="build.ivy.lib.dir" location="${build.ivy.dir}/lib" />
+    <property name="build.ivy.report.dir" location="${build.ivy.dir}/report" />
+    <property name="build.ivy.maven.dir" location="${build.ivy.dir}/maven" />
+    <property name="module" value="hdfs" />
+    <property name="build.ivy.maven.pom" 
+      location="${build.ivy.maven.dir}/hadoop-${module}-${hadoop.version}.pom" />
+    <property name="build.ivy.maven.jar" 
+      location="${build.ivy.maven.dir}/hadoop-${module}-${hadoop.version}.jar" />
+
+    <!--this is the naming policy for artifacts we want pulled down-->
+    <property name="ivy.artifact.retrieve.pattern"
+      value="[conf]/[artifact]-[revision].[ext]"/>
+    <!--this is how artifacts that get built are named-->
+    <property name="ivy.publish.pattern"
+      value="hadoop-[revision]-core.[ext]"/>
+    <property name="hadoop.jar"
+      location="${build.dir}/hadoop-${module}-${hadoop.version}.jar" />
+
+    <!--preset to build down; puts us in control of version naming-->
+    <presetdef name="delegate">
+      <ant antfile="build.xml" inheritall="false" inheritrefs="false" >
+        <property name="version" value="${hadoop.version}"/>
+      </ant>
+    </presetdef>
+    <!--preset to build down; puts us in control of version naming-->
+    <presetdef name="delegate2">
+      <subant antfile="build.xml" buildpath="." inheritall="false" inheritrefs="false" >
+        <property name="version" value="${hadoop.version}"/>
+      </subant>
+    </presetdef>
+
+    <!--preset to copy with ant property expansion (and always overwrite)-->
+    <presetdef name="expandingcopy" >
+    <copy overwrite="true">
+      <filterchain>
+        <expandproperties/>
+      </filterchain>
+    </copy>
+  </presetdef>
+  </target>
+
+
+  <target name="ivy-init-dirs" depends="ivy-init-properties" >
+    <mkdir dir="${build.ivy.dir}" />
+    <mkdir dir="${build.ivy.lib.dir}" />
+    <mkdir dir="${build.ivy.report.dir}" />
+    <mkdir dir="${build.ivy.maven.dir}" />
+  </target>
+
+
+  <target name="clean"  depends="ivy-init-properties"
+    description="Clean the output directories" >
+    <delegate target="clean" />
+  </target>
+
+
+  <target name="jar"  depends="ivy-init-dirs"
+    description="build the JAR">
+    <delegate target="jar" />
+  </target>
+
+  <!--
+    This looks for Ivy on the classpath, and is used to skip reloading it if found.
+    It looks for an ivy-2.0 file.
+  -->
+  <target name="ivy-probe-antlib" >
+    <condition property="ivy.found">
+      <typefound uri="antlib:org.apache.ivy.ant" name="cleancache"/>
+    </condition>
+  </target>
+
+
+  <!--
+  To avoid Ivy leaking things across big projects, always load Ivy in the same classloader.
+  Also note how we skip loading Ivy if it is already there, just to make sure all is well.
+  -->
+  <target name="ivy-init-antlib" depends="ivy-init-properties,ivy-init-dirs,ivy-probe-antlib" unless="ivy.found">
+
+    <typedef uri="antlib:org.apache.ivy.ant" onerror="fail"
+      loaderRef="ivyLoader">
+      <classpath>
+        <pathelement location="${ivy.jar}"/>
+      </classpath>
+    </typedef>
+    <fail >
+      <condition >
+        <not>
+          <typefound uri="antlib:org.apache.ivy.ant" name="cleancache"/>
+        </not>
+      </condition>
+      You need Apache Ivy 2.0 or later from http://ant.apache.org/
+      It could not be loaded from ${ivy.jar}
+    </fail>
+  </target>
+
+
+  <target name="ivy-init" depends="ivy-init-antlib" >
+
+    <!--Configure Ivy by reading in the settings file
+        If anyone has already read in a settings file into this settings ID, it gets priority
+    -->
+    <ivy:configure settingsId="hadoop.ivy.settings" file="${ivysettings.xml}" override="false"/>
+
+  </target>
+
+  <target name="ivy-resolve" depends="ivy-init">
+    <ivy:resolve settingsRef="hadoop.ivy.settings"/>
+  </target>
+
+  <target name="ivy-retrieve" depends="ivy-resolve"
+    description="Retrieve all Ivy-managed artifacts for the different configurations">
+    <ivy:retrieve settingsRef="hadoop.ivy.settings"
+      pattern="${build.ivy.lib.dir}/${ivy.artifact.retrieve.pattern}" sync="true" />
+  </target>
+
+  <target name="ivy-report" depends="ivy-resolve"
+    description="Generate">
+    <ivy:report todir="${build.ivy.report.dir}" settingsRef="hadoop.ivy.settings"/>
+    <echo>
+      Reports generated:
+${build.ivy.report.dir}
+    </echo>
+  </target>
+
+  <target name="assert-hadoop-jar-exists" depends="ivy-init">
+    <fail>
+      <condition >
+        <not>
+          <available file="${hadoop.jar}" />
+        </not>
+      </condition>
+      Not found: ${hadoop.jar}
+      Please run the target "jar" in the main build file
+    </fail>
+
+  </target>
+
+  <target name="ready-to-publish" depends="jar,assert-hadoop-jar-exists,ivy-resolve"/>
+
+  <target name="ivy-publish-local" depends="ready-to-publish">
+    <ivy:publish
+      settingsRef="hadoop.ivy.settings"
+      resolver="local"
+      pubrevision="${hadoop.version}"
+      overwrite="true"
+      artifactspattern="${build.dir}/${ivy.publish.pattern}" />
+  </target>
+
+
+  <!-- this is here for curiosity, to see how well the makepom task works
+  Answer: it depends whether you want transitive dependencies excluded or not
+  -->
+  <target name="makepom" depends="ivy-resolve">
+    <ivy:makepom settingsRef="hadoop.ivy.settings"
+      ivyfile="ivy.xml"
+      pomfile="${build.ivy.maven.dir}/generated.pom">
+      <ivy:mapping conf="default" scope="default"/>
+      <ivy:mapping conf="master"  scope="master"/>
+      <ivy:mapping conf="runtime" scope="runtime"/>
+    </ivy:makepom>
+  </target>
+
+
+  <target name="copy-jar-to-maven" depends="ready-to-publish">
+    <copy file="${hadoop.jar}"
+      tofile="${build.ivy.maven.jar}"/>
+    <checksum file="${build.ivy.maven.jar}" algorithm="md5"/>
+  </target>
+
+  <target name="copypom" depends="ivy-init-dirs">
+    <expandingcopy file="ivy/hadoop-core.pom"
+      tofile="${build.ivy.maven.pom}"/>
+    <checksum file="${build.ivy.maven.pom}" algorithm="md5"/>
+  </target>
+
+  <target name="maven-artifacts" depends="copy-jar-to-maven,copypom" />
+
+  <target name="mvn-install" depends="ready-to-publish">
+    <delegate target="mvn-install" />
+  </target>
+
+  <target name="published" depends="ivy-publish-local,mvn-install">
+
+  </target>
+
+  <target name="ready-to-test" depends="ivy-init-dirs">
+    <property name="test.data.dir" location="${build.dir}/test/data" />
+    <property name="test.reports.dir" location="${build.dir}/test/reports" />
+    <mkdir dir="${test.data.dir}" />
+    <mkdir dir="${test.reports.dir}" />
+  </target>
+
+  <target name="testjob.jar"  depends="ready-to-test">
+    <delegate2 target="jar-test"
+        failonerror="true">
+    </delegate2>
+  </target>
+
+
+  <target name="junit"  depends="ready-to-test,testjob.jar"
+      description="run the junit tests and generate an XML report">
+    <delegate2 target="test-core"
+        failonerror="false">
+      <property name="test.junit.output.format" value="xml" />
+      <property name="test.build.dir" value="${test.data.dir}"/>
+    </delegate2>
+  </target>
+
+  <!-- generate a junit report. 
+  tip: you can run this while junit is still going on-->
+  <target name="junitreport"  depends="ready-to-test">
+    <junitreport todir="${test.reports.dir}">
+      <fileset dir="${test.data.dir}">
+        <include name="TEST-*.xml"/>
+      </fileset>
+      <report format="frames" todir="${test.reports.dir}"/>
+    </junitreport>
+    <echo>reports in ${test.reports.dir}/index.html</echo>
+  </target>
+
+  <target name="tested" depends="junit,junitreport" />
+
+
+  <target name="ivy-purge-cache" depends="ivy-init-properties"
+      description="Purge the Ivy and maven caches">
+    <delete dir="${user.home}/.ivy/cache/org.apache.hadoop/" />
+    <delete dir="${user.home}/.ivy2/cache/org.apache.hadoop/" />
+    <delete dir="${user.home}/.m2/repository/org/apache/hadoop/" />
+  </target>
+</project>
diff --git a/src/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java b/src/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
index a01ac9b..4c6ac4a 100644
--- a/src/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
+++ b/src/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
@@ -61,12 +61,12 @@ import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.conf.Configured;
 import org.apache.hadoop.fs.CommonConfigurationKeys;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.LocalFileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.hdfs.DFSConfigKeys;
 import org.apache.hadoop.hdfs.DFSUtil;
 import org.apache.hadoop.hdfs.HDFSPolicyProvider;
 import org.apache.hadoop.hdfs.HdfsConfiguration;
@@ -130,6 +130,7 @@ import org.apache.hadoop.util.DiskChecker;
 import org.apache.hadoop.util.DiskChecker.DiskErrorException;
 import org.apache.hadoop.util.DiskChecker.DiskOutOfSpaceException;
 import org.apache.hadoop.util.GenericOptionsParser;
+import org.apache.hadoop.util.LifecycleService;
 import org.apache.hadoop.util.ReflectionUtils;
 import org.apache.hadoop.util.ServicePlugin;
 import org.apache.hadoop.util.StringUtils;
@@ -168,7 +169,7 @@ import org.mortbay.util.ajax.JSON;
  *
  **********************************************************/
 @InterfaceAudience.Private
-public class DataNode extends Configured 
+public class DataNode extends LifecycleService 
     implements InterDatanodeProtocol, ClientDatanodeProtocol, FSConstants,
     DataNodeMXBean {
   public static final Log LOG = LogFactory.getLog(DataNode.class);
@@ -200,6 +201,8 @@ public class DataNode extends Configured
                                                    ) throws IOException {
     return NetUtils.createSocketAddr(target);
   }
+  public DatanodeProtocol namenode = null;
+  public DatanodeRegistration dnRegistration = null;
   
   /**
    * Manages he BPOfferService objects for the data node.
@@ -386,6 +389,13 @@ public class DataNode extends Configured
   /**
    * Create the DataNode given a configuration and an array of dataDirs.
    * 'dataDirs' is where the blocks are stored.
+   * <p>
+   * Important: this constructor does not start
+   * the node, merely initializes it
+   *
+   * @param conf     configuration to use
+   * @param dataDirs list of directories that may be used for data
+   * @throws IOException if the construction fails
    */
   DataNode(final Configuration conf, 
            final AbstractList<File> dataDirs) throws IOException {
@@ -401,11 +411,29 @@ public class DataNode extends Configured
            final SecureResources resources) throws IOException {
     super(conf);
 
+    //cache values used when starting up
+    this.dataDirs = dataDirs;
+    this.secureResources = resources;
+  }
+
+/////////////////////////////////////////////////////
+// Lifecycle
+/////////////////////////////////////////////////////
+
+  /**
+   * Start any work (in separate threads)
+   *
+   * @throws IOException for any startup failure
+   * @throws InterruptedException if the thread was interrupted on startup
+   */
+  @Override
+  public void serviceStart() throws IOException, InterruptedException {
+    //cache this datanode as the last one started
     DataNode.setDataNode(this);
     
     try {
       hostName = getHostName(conf);
-      startDataNode(conf, dataDirs, resources);
+      startDataNode(conf, dataDirs, secureResources);
     } catch (IOException ie) {
       shutdown();
       throw ie;
@@ -1595,23 +1623,36 @@ public class DataNode extends Configured
   }
 
   /**
+   * Shut down this instance of the datanode. Returns only after shutdown is
+   * complete.
+   */
+  public void shutdown() {
+    closeQuietly();
+  }
+
+  /**
    * Shut down this instance of the datanode.
    * Returns only after shutdown is complete.
    * This method can only be called by the offerService thread.
    * Otherwise, deadlock might occur.
    */
-  public void shutdown() {
-    if (plugins != null) {
-      for (ServicePlugin p : plugins) {
-        try {
-          p.stop();
-          LOG.info("Stopped plug-in " + p);
-        } catch (Throwable t) {
-          LOG.warn("ServicePlugin " + p + " could not be stopped", t);
+  @Override
+  protected void serviceClose() throws IOException {
+    synchronized (this) {
+      //disable the should run flag first, so that everything out there starts
+      //to shut down
+      this.shouldRun = false;
+      if (plugins != null) {
+        for (ServicePlugin p : plugins) {
+          try {
+            p.stop();
+            LOG.info("Stopped plug-in " + p);
+          } catch (Throwable t) {
+            LOG.warn("ServicePlugin " + p + " could not be stopped", t);
+          }
         }
       }
-    }
-    
+
     shutdownPeriodicScanners();
     
     if (infoServer != null) {
@@ -1677,6 +1718,7 @@ public class DataNode extends Configured
     if (myMetrics != null) {
       myMetrics.shutdown();
     }
+    }
   }
   
   
@@ -2146,7 +2188,14 @@ public class DataNode extends Configured
     ArrayList<File> dirs = getDataDirsFromURIs(dataDirs, localFS, permission);
 
     assert dirs.size() > 0 : "number of data directories should be > 0";
-    return new DataNode(conf, dirs, resources);
+    if (dirs.size() <= 0) {
+      LOG.error("All directories in "
+                    + DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY + " are invalid.");
+      return null;
+    }
+    DataNode dataNode = new DataNode(conf, dirs, resources);
+    LifecycleService.startService(dataNode);
+    return dataNode;
   }
 
   // DataNode ctor expects AbstractList instead of List or Collection...
@@ -2179,8 +2228,13 @@ public class DataNode extends Configured
   }
 
   @Override
+  public String getServiceName() {
+    return "DataNode";
+  }
+
+  @Override
   public String toString() {
-    return "DataNode{data=" + data + ", localName='" + getMachineName()
+    return getServiceName() +"{data=" + data + ", localName='" + getMachineName()
         + "', storageID='" + getStorageId() + "', xmitsInProgress="
         + xmitsInProgress.get() + "}";
   }
diff --git a/src/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java b/src/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java
index d078315..0b2cf86 100644
--- a/src/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java
+++ b/src/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java
@@ -147,8 +147,17 @@ public class BackupNode extends NameNode {
     runCheckpointDaemon(conf);
   }
 
+  /**
+   * {@inheritDoc}
+   * <p/>
+   * When shutting down, this service shuts down the checkpoint manager.
+   * If registered to a namenode, it reports that it is shutting down
+   * via {@link NameNode#errorReport(NamenodeRegistration, int, String)}
+   *
+   * @throws IOException for any IO problem
+   */
   @Override // NameNode
-  public void stop() {
+  protected void serviceClose() throws IOException {
     if(checkpointManager != null) {
       // Prevent from starting a new checkpoint.
       // Checkpoints that has already been started may proceed until 
@@ -176,7 +185,17 @@ public class BackupNode extends NameNode {
       checkpointManager = null;
     }
     // Stop name-node threads
-    super.stop();
+    super.serviceClose();
+  }
+
+  /**
+   * {@inheritDoc}
+   *
+   * @return "BackupNode"
+   */
+  @Override  // NameNode
+  public String getServiceName() {
+    return "BackupNode";
   }
 
   /////////////////////////////////////////////////////
diff --git a/src/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java b/src/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java
index 6381065..9830dc5 100644
--- a/src/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java
+++ b/src/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java
@@ -99,6 +99,7 @@ import org.apache.hadoop.security.authorize.AuthorizationException;
 import org.apache.hadoop.security.authorize.ProxyUsers;
 import org.apache.hadoop.security.authorize.RefreshAuthorizationPolicyProtocol;
 import org.apache.hadoop.security.token.SecretManager.InvalidToken;
+import org.apache.hadoop.util.LifecycleServiceWithWorkers;
 import org.apache.hadoop.security.token.Token;
 import org.apache.hadoop.util.ServicePlugin;
 import org.apache.hadoop.util.StringUtils;
@@ -138,7 +139,8 @@ import org.apache.hadoop.util.StringUtils;
  * state, for example partial blocksMap etc.
  **********************************************************/
 @InterfaceAudience.Private
-public class NameNode implements NamenodeProtocols, FSConstants {
+public class NameNode extends LifecycleServiceWithWorkers 
+    implements NamenodeProtocols, FSConstants {
   static{
     Configuration.addDefaultResource("hdfs-default.xml");
     Configuration.addDefaultResource("hdfs-site.xml");
@@ -556,7 +558,7 @@ public class NameNode implements NamenodeProtocols, FSConstants {
   }
 
   /**
-   * Start NameNode.
+   * Create a NameNode.
    * <p>
    * The name-node can be started with one of the following startup options:
    * <ul> 
@@ -588,14 +590,83 @@ public class NameNode implements NamenodeProtocols, FSConstants {
 
   protected NameNode(Configuration conf, NamenodeRole role) 
       throws IOException { 
+    super(conf);
     this.role = role;
+  }
+
+
+  /**
+   * This method does all the startup. It is invoked from {@link #start()} when
+   * needed.
+   *
+   * This implementation delegates all the work to the (overridable)
+   * {@link #initialize(Configuration)} method, then calls
+   * {@link #setServiceState(ServiceState)} to mark the service as live.
+   * Any subclasses that do not consider themsevles to be live once 
+   * any subclassed initialize method has returned should override the method
+   * {@link #goLiveAtTheEndOfStart()} to change that behavior.
+   * @throws IOException for any problem that prevented startup.
+   * @throws InterruptedException if the thread was interrupted on startup
+   */
+  @Override //LifecycleService
+  protected void serviceStart() throws IOException, InterruptedException {
     try {
-      initializeGenericKeys(conf);
-      initialize(conf);
+      initializeGenericKeys(getConf());
+      initialize(getConf());
     } catch (IOException e) {
       this.stop();
       throw e;
     }
+    if (goLiveAtTheEndOfStart()) {
+      setServiceState(ServiceState.LIVE);
+    }
+  }
+
+  /**
+     * The toString operator returns the super class name/id, and the state. This
+     * gives all services a slightly useful message in a debugger or test report
+     *
+     * @return a string representation of the object.
+     */
+    @Override
+    public String toString() {
+        return super.toString()
+                + (httpAddress != null ? (" at " + httpAddress + " , ") : "")
+                + (server != null ? (" IPC " + server.getListenerAddress()) : "");
+    }
+
+    /////////////////////////////////////////////////////
+    // Service Lifecycle and other methods
+    /////////////////////////////////////////////////////
+
+    /**
+     * {@inheritDoc}
+     *
+     * @return "NameNode"
+     */
+    @Override //LifecycleService
+    public String getServiceName() {
+        return "NameNode";
+    }
+
+    /**
+     * Get the current number of workers
+     * @return the worker count
+     */
+    @Override //LifecycleServiceWithWorkers
+    public int getLiveWorkerCount() {
+        return getNamesystem() != null?
+                getNamesystem().heartbeats.size()
+                : 0;
+    }
+  /**
+   * Override point: should the NameNode enter the live state at the end of
+   * the {@link #serviceStart()} operation?
+   * @return true if the service should enter the live state at this point,
+   * false to leave the service in its current state.
+   */
+  protected boolean goLiveAtTheEndOfStart() {
+    return true;
   }
 
   /**
@@ -611,8 +682,24 @@ public class NameNode implements NamenodeProtocols, FSConstants {
 
   /**
    * Stop all NameNode threads and wait for all to finish.
+   * <p/>
+   * Retained for backwards compatibility.
+   */
+  public final void stop() {
+    closeQuietly();
+  }
+
+  /**
+   * {@inheritDoc}
+   * <p/>
+   * To shut down, this service stops all NameNode threads and
+   * waits for them to finish. It also stops the metrics.
+   * @throws IOException for any IO problem
    */
-  public void stop() {
+  @Override //LifecycleService
+  protected synchronized void serviceClose() throws IOException {
+    LOG.info("Closing " + getServiceName());
+
     synchronized(this) {
       if (stopRequested)
         return;
@@ -633,14 +720,24 @@ public class NameNode implements NamenodeProtocols, FSConstants {
       LOG.error(StringUtils.stringifyException(e));
     }
     if(namesystem != null) namesystem.close();
-    if(emptier != null) emptier.interrupt();
-    if(server != null) server.stop();
-    if(serviceRpcServer != null) serviceRpcServer.stop();
+    if(emptier != null) {
+      emptier.interrupt();
+      emptier = null;
+    }
+    if(server != null) {
+      server.stop();
+      //server = null;
+    }
+    if(serviceRpcServer != null) {
+      serviceRpcServer.stop();
+      serviceRpcServer = null;
+    }
     if (myMetrics != null) {
       myMetrics.shutdown();
     }
     if (namesystem != null) {
       namesystem.shutdown();
+      namesystem = null;
     }
   }
 
@@ -1634,9 +1731,13 @@ public class NameNode implements NamenodeProtocols, FSConstants {
         return null; // avoid javac warning
       case BACKUP:
       case CHECKPOINT:
-        return new BackupNode(conf, startOpt.toNodeRole());
+        BackupNode backupNode = new BackupNode(conf, startOpt.toNodeRole());
+        startService(backupNode);
+        return backupNode;
       default:
-        return new NameNode(conf);
+        NameNode nameNode = new NameNode(conf);
+        startService(nameNode);
+        return nameNode;
     }
   }
 
diff --git a/src/test/hdfs/org/apache/hadoop/hdfs/server/namenode/TestReplicationPolicy.java b/src/test/hdfs/org/apache/hadoop/hdfs/server/namenode/TestReplicationPolicy.java
index 897c2db..a4cfda9 100644
--- a/src/test/hdfs/org/apache/hadoop/hdfs/server/namenode/TestReplicationPolicy.java
+++ b/src/test/hdfs/org/apache/hadoop/hdfs/server/namenode/TestReplicationPolicy.java
@@ -63,6 +63,7 @@ public class TestReplicationPolicy extends TestCase {
       CONF.set(DFSConfigKeys.DFS_NAMENODE_HTTP_ADDRESS_KEY, "0.0.0.0:0");
       GenericTestUtils.formatNamenode(CONF);
       namenode = new NameNode(CONF);
+      NameNode.startService(namenode);
     } catch (IOException e) {
       e.printStackTrace();
       throw (RuntimeException)new RuntimeException().initCause(e);
